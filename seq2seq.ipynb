{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import random\n",
    "\n",
    "# for accesing files in the directory\n",
    "import glob\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "# Input batch X (acoustic) to be passed for training\n",
    "path = './asr_data/train/*.npy'\n",
    "files = glob.glob(path)\n",
    "\n",
    "# List of input batches\n",
    "X = []\n",
    "\n",
    "cnt = 0\n",
    "acoustics = []\n",
    "maxL = 0\n",
    "for i,name in enumerate(files):    \n",
    "    ac = np.load(name)\n",
    "    acoustics.append(np.transpose(ac))\n",
    "    maxL = max(maxL, ac.shape[1])\n",
    "    if cnt==31:\n",
    "        # Acoustic inputs should be FloatTensors: B x L x 20 \n",
    "        batch_X = torch.zeros(32,maxL,20)\n",
    "        for j,ac in enumerate(acoustics):\n",
    "            batch_X[j, :ac.shape[0], :] = torch.from_numpy(ac)\n",
    "        X.append(batch_X)\n",
    "        cnt = 0\n",
    "        acoustics = []\n",
    "        maxL = 0\n",
    "    cnt += 1\n",
    "\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# Test Input batch to be passed for calculating testing CER\n",
    "path = './asr_data/test/*.npy'\n",
    "files = glob.glob(path)\n",
    "\n",
    "# List of input batches\n",
    "cer_testX = []\n",
    "\n",
    "cnt = 0\n",
    "acoustics = []\n",
    "maxL = 0\n",
    "for i,name in enumerate(files):    \n",
    "    ac = np.load(name)\n",
    "    acoustics.append(np.transpose(ac))\n",
    "    maxL = max(maxL, ac.shape[1])\n",
    "    if cnt==31:\n",
    "        # Acoustic inputs should be FloatTensors: B x L x 20 \n",
    "        batch_X = torch.zeros(32,maxL,20)\n",
    "        for j,ac in enumerate(acoustics):\n",
    "            batch_X[j, :ac.shape[0], :] = torch.from_numpy(ac)\n",
    "        cer_testX.append(batch_X)\n",
    "        cnt = 0\n",
    "        acoustics = []\n",
    "        maxL = 0\n",
    "    cnt += 1\n",
    "\n",
    "print(len(cer_testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label/Character encoding\n",
    "chars = ['<PAD>','<SOS>', '<EOS>',' ',\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\", \\\n",
    "         \"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\", \"v\",\"w\",\"x\",\"y\",\"z\"]\n",
    "dict(enumerate(chars))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:i for i,ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "# Input Label batch to be passed for Training\n",
    "path = './asr_data/train/*.txt'\n",
    "files = glob.glob(path)\n",
    "\n",
    "# List of label baatches and associated masks\n",
    "Y, M = [], []\n",
    "\n",
    "labels = []\n",
    "cnt = 0\n",
    "maxL_Y = 0\n",
    "for name in files:\n",
    "    with open(name) as f:\n",
    "        text = f.read()\n",
    "    lb = ['<SOS>']+list(text)+['<EOS>']\n",
    "    labels.append(lb)\n",
    "    maxL_Y = max(maxL_Y, len(lb))\n",
    "    if cnt==31:\n",
    "        # As <PAD> == 0,\n",
    "        batch_Y = torch.zeros(32,maxL_Y)\n",
    "        mask_Y = torch.zeros(32,maxL_Y)\n",
    "        for i,lb in enumerate(labels):\n",
    "            for j,ch in enumerate(lb):\n",
    "                batch_Y[i,j] = char2int[ch]\n",
    "                mask_Y[i,j] = 1\n",
    "        # Labels should be LongTensors: B x maxL \n",
    "        batch_Y = batch_Y.type(torch.LongTensor)\n",
    "        Y.append(batch_Y)\n",
    "        M.append(mask_Y)\n",
    "        cnt = 0\n",
    "        maxL_Y = 0\n",
    "        labels = []\n",
    "    cnt+=1  \n",
    "\n",
    "print(len(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "# Test Label batch to be used for testing-CER calculation\n",
    "path = './asr_data/test/*.txt'\n",
    "files = glob.glob(path)\n",
    "\n",
    "# List of label baatches and associated masks\n",
    "cer_Y, cer_M = [], []\n",
    "\n",
    "labels = []\n",
    "cnt = 0\n",
    "maxL_Y = 0\n",
    "for name in files:\n",
    "    with open(name) as f:\n",
    "        text = f.read()\n",
    "    lb = ['<SOS>']+list(text)+['<EOS>']\n",
    "    labels.append(lb)\n",
    "    maxL_Y = max(maxL_Y, len(lb))\n",
    "    if cnt==31:\n",
    "        # As <PAD> == 0,\n",
    "        batch_Y = torch.zeros(32,maxL_Y)\n",
    "        mask_Y = torch.zeros(32,maxL_Y)\n",
    "        for i,lb in enumerate(labels):\n",
    "            for j,ch in enumerate(lb):\n",
    "                batch_Y[i,j] = char2int[ch]\n",
    "                mask_Y[i,j] = 1\n",
    "        # Labels should be LongTensors: B x maxL \n",
    "        batch_Y = batch_Y.type(torch.LongTensor)\n",
    "        cer_Y.append(batch_Y)\n",
    "        cer_M.append(mask_Y)\n",
    "        cnt = 0\n",
    "        maxL_Y = 0\n",
    "        labels = []\n",
    "    cnt+=1  \n",
    "\n",
    "print(len(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class Encoder, which inherits the properties and methods from the parent class nn.Module\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = 20   # given 20 x L acoustic inputs\n",
    "        self.hidden_size = 128\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first = True)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # input to LSTM = B x L x 20\n",
    "        output, hidden = self.lstm(inp)\n",
    "        # output, (h_n, c_n) = self.lstm(embedding, (h, c)) ----- (h,c) initialized to zero\n",
    "        # output size = B x Lx 128\n",
    "        # (h,c) are from the last time step: both have size [1,B,128]\n",
    "        # return the last hidden output 1 x B x H\n",
    "        return (hidden[0][0,:,:],hidden[1][0,:,:])\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_size = 256\n",
    "        self.decoder_hidden_size = 128\n",
    "        self.embedding = nn.Embedding(vocab_size, self.embedding_size)        \n",
    "        # NOTE: Use LSTM Cell here instead if you want to control the hidden state at each time step.\n",
    "        self.lstm = nn.LSTMCell(self.embedding_size, self.decoder_hidden_size)\n",
    "        self.lin = nn.Linear(self.decoder_hidden_size, vocab_size)\n",
    "       \n",
    "    def forward_step(self, word_embedding, hidden):\n",
    "        output, new_cell_state = self.lstm(word_embedding, hidden)\n",
    "        new_hidden = output\n",
    "        vocab_distrbtn = F.softmax(self.lin(output), dim=1)\n",
    "        return vocab_distrbtn, (new_hidden, new_cell_state)\n",
    "        \n",
    "    def forward(self, inpt, encoder_hidden, mask_Y, beta):\n",
    "        t_max = inpt.shape[1]\n",
    "        loss = 0\n",
    "        # SOS = 1\n",
    "        word = inpt[:,0]\n",
    "        word_embedding = self.embedding(word)\n",
    "\n",
    "        hidden = encoder_hidden\n",
    "        for t in range(t_max-1):\n",
    "            vocab_dist, hidden = self.forward_step(word_embedding, hidden)  # vocab_dist = B x V = 10 x 30\n",
    "            word = torch.argmax(vocab_dist, dim=1)   # word = B x 1\n",
    "            \n",
    "            # DAgger policy = beta*oracle + (1-beta)*model\n",
    "            u = random.uniform(0, 1)\n",
    "            if u<=beta:\n",
    "                # Teacher Forcing\n",
    "                word_embedding = self.embedding(inpt[:,t+1])\n",
    "            else:\n",
    "                # Model's output as next input\n",
    "                word_embedding = self.embedding(word)\n",
    "            \n",
    "            # Cross Entropy Loss\n",
    "            # ground truth B x 1 is the char at time step t+1 or t+1th column in B x L = 32 x L\n",
    "            true_label = inpt[:,t+1]            \n",
    "            # one hot encode the true label # B x 1 = 32 x 1 --> 32 x 30\n",
    "            onehot = torch.zeros((32,30))\n",
    "            for i in range(32):\n",
    "                onehot[i][true_label[i]]=1\n",
    "            # Cross entropy loss: vocab_dist 32 x 30, onehot 32 x 30\n",
    "            NLL = (-1)*torch.log(vocab_dist)\n",
    "            ce_loss = torch.sum(NLL*onehot, dim=1)\n",
    "            loss += torch.sum(ce_loss*mask_Y[:,t])\n",
    "            \n",
    "        # averaged loss over the entire batch (except padding)\n",
    "        return loss/torch.sum(mask_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, n_epochs, learning_rate, X, Y, M, cer_testX, cer_Y, cer_M):\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    decoder_optimizer = torch.optim.Adam(decoder.parameters())\n",
    "    # Default parameters: lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    train_cerloss = []\n",
    "    test_cerloss = []\n",
    "    \n",
    "#     beta = 1   # All oracle\n",
    "#     beta = 0   # All Model\n",
    "#     beta = 0.75\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = len(X)\n",
    "        num_test_batches = len(cer_testX)\n",
    "        \n",
    "        \n",
    "        # beta = beta - 0.05\n",
    "        beta = np.exp(-epoch)\n",
    "\n",
    "        for i, batch in enumerate(X):\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            encoder_hidden = encoder(X[i])\n",
    "            loss = decoder(Y[i], encoder_hidden, M[i], beta)\n",
    "\n",
    "            loss.backward(retain_graph=True)\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # Testing Loss\n",
    "        test_loss = 0\n",
    "        for j, batch in enumerate(cer_testX):\n",
    "            encoder_hidden = encoder(cer_testX[j])\n",
    "            t_loss = decoder(cer_Y[j], encoder_hidden, cer_M[j], beta)\n",
    "            test_loss+=t_loss\n",
    "                      \n",
    "        print(\"Epoch {}: Training Loss: {}, Testing Loss: {}\".format(epoch, epoch_loss/num_batches, \\\n",
    "                                                                     test_loss/num_test_batches))\n",
    "        train_cerloss.append(epoch_loss/num_batches)\n",
    "        test_cerloss.append(test_loss/num_test_batches)\n",
    "    return train_cerloss, test_cerloss\n",
    "\n",
    "\n",
    "def testDecoder(decoder, encoder_hidden):\n",
    "    t_max = 80\n",
    "    prediction = []\n",
    "    # SOS = 1\n",
    "    word = torch.ones(1)\n",
    "    word = word.type(torch.LongTensor)\n",
    "    word_embedding = decoder.embedding(word)\n",
    "\n",
    "    # Feed in the encoder_hidden\n",
    "    hidden = encoder_hidden\n",
    "    for t in range(t_max-1):\n",
    "        vocab_dist, hidden = decoder.forward_step(word_embedding, hidden)  # vocab_dist = B x V = 10 x 30\n",
    "        word = torch.argmax(vocab_dist, dim=1)   # word = B x 1\n",
    "        if word==2:\n",
    "            break\n",
    "        prediction.append(word)\n",
    "        # Model's output as next input\n",
    "        word_embedding = decoder.embedding(word)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def evaluate(encoder, decoder, testX, testY):\n",
    "\n",
    "    num_batches = len(testX)\n",
    "    ret = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(testX):\n",
    "            encoder_hidden = encoder(testX[i])\n",
    "            prediction = testDecoder(decoder, encoder_hidden)\n",
    "            sentence = []\n",
    "            for ii in prediction:\n",
    "                sentence.append(int2char[int(ii)])\n",
    "            ret.append(''.join(sentence))\n",
    "#             print(''.join(sentence))\n",
    "    return ret\n",
    "#     print(total_acc / num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss: 2.7066415602510627, Testing Loss: 2.3830313682556152\n",
      "Epoch 1: Training Loss: 2.8563515652309763, Testing Loss: 2.791280508041382\n",
      "Epoch 2: Training Loss: 2.8817535042762756, Testing Loss: 2.8771705627441406\n",
      "Epoch 3: Training Loss: 2.902845165946267, Testing Loss: 2.9045560359954834\n"
     ]
    }
   ],
   "source": [
    "EMB_SIZE = 256\n",
    "HIDDEN_SIZE = 128\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "VOCAB_SIZE = 30    #26 + space,sos,eos,pad\n",
    "\n",
    "encoder = Encoder(VOCAB_SIZE)\n",
    "decoder = Decoder(VOCAB_SIZE)\n",
    "cer_tr, cer_ts = train(encoder, decoder, EPOCHS, LEARNING_RATE, X, Y, M, cer_testX, cer_Y, cer_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Training and Testing CER Plots\n",
    "fig, ax = plt.subplots(figsize = (8,5));\n",
    "\n",
    "\n",
    "# plt.ylim(0,3.2)\n",
    "# plt.xlim(0.5,10)\n",
    "plt.plot(range(EPOCHS), cer_tr, label = 'Training')\n",
    "plt.plot(range(EPOCHS), cer_ts, label = 'Testing')\n",
    "ax.legend()\n",
    "ax.set_title('CER loss, beta = 0 (All Model)', fontsize=16)\n",
    "ax.set_xlabel('Epoch', fontsize=16);\n",
    "ax.set_ylabel('CE loss', fontsize=16);\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504\n",
      "504\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "# Test Input(acoustic)\n",
    "path = './asr_data/test/*.npy'\n",
    "files = glob.glob(path)\n",
    "\n",
    "# List of input batches\n",
    "testX = []\n",
    "acoustics = []\n",
    "cnt = 0\n",
    "for i,name in enumerate(files):    \n",
    "    ac = np.load(name)\n",
    "#     print(ac.shape)\n",
    "    # Acoustic inputs should be FloatTensors: B x L x 20 \n",
    "    a = torch.transpose(torch.from_numpy(ac), 0,1)\n",
    "#     print(a.shape)\n",
    "    a = a.view(1,a.shape[0], a.shape[1])\n",
    "#     print(a.shape)\n",
    "    testX.append(a)\n",
    "#     if cnt==4: break\n",
    "#     cnt+=1\n",
    "print(len(testX))\n",
    "\n",
    "# Training Label batch\n",
    "path = './asr_data/test/*.txt'\n",
    "files = glob.glob(path)\n",
    "\n",
    "true_label = []\n",
    "cnt = 0\n",
    "for name in files:\n",
    "    with open(name) as f:\n",
    "        text = f.read()\n",
    "    true_label.append(text)\n",
    "#     if cnt==4: break\n",
    "#     cnt+=1\n",
    "print(len(true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7907876230661046\n"
     ]
    }
   ],
   "source": [
    "pred = evaluate(encoder, decoder, testX, 0)\n",
    "\n",
    "cer = 0\n",
    "for i in range(len(testX)):\n",
    "#     print(pred[i])\n",
    "#     print(true_label[i])\n",
    "    cer += edit_distance(pred[i], true_label[i]) / len(pred[i])\n",
    "avg_cer = cer/len(testX)\n",
    "print(avg_cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
